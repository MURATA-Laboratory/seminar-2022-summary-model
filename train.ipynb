{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBQXJwDV06u"
      },
      "source": [
        "# 短縮モデル 学習ノートブック\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9xq3_i3V06z"
      },
      "source": [
        "## Mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSCjfVL9V060",
        "outputId": "e63be4b3-7feb-42ab-f7c6-c466cf637d77"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6VKp5XV062"
      },
      "source": [
        "## Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-84t3vn-WXGD",
        "outputId": "93ce99cb-af94-4808-c98b-0920d3e1ae20"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install wandb\n",
        "!pip install pytorch-lightning\n",
        "!pip install rich\n",
        "!pip install python-box\n",
        "!pip install sentencepiece\n",
        "!pip install \"sacrebleu[ja]<2.0.0\"\n",
        "!pip install janome\n",
        "!pip install sumeval\n",
        "!pip install unidic-lite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zMES_u3V063"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "from box import Box\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import RichProgressBar\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import torch\n",
        "import torchmetrics\n",
        "import wandb\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "from sumeval.metrics.bleu import BLEUCalculator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw9osNZOV065"
      },
      "source": [
        "## Wandb Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "J1U9dKkuV065",
        "outputId": "5f3b9ec3-ee51-44e6-aa6f-cbf0d65f249d"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObOFR636V066"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FgOqvTcV067"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET_PATH = \"/content/drive/MyDrive/MurataLab/summary/train_dataset.csv\"\n",
        "TEST_DATASET_PATH = \"/content/drive/MyDrive/MurataLab/summary/test_dataset.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3ocWDTV067"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhzpMGbwV068"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    TEXT_COLUMN = \"text\"\n",
        "    SUMMARY_COLUMN = \"summary\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, text_max_token_len, summary_max_token_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_max_token_len = text_max_token_len\n",
        "        self.summary_max_token_len = summary_max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_row = self.data.iloc[index]\n",
        "        text = data_row[self.TEXT_COLUMN]\n",
        "        summary = data_row[self.SUMMARY_COLUMN]\n",
        "\n",
        "        text_encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.text_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        summary_encoding = self.tokenizer.encode_plus(\n",
        "            summary,\n",
        "            max_length=self.summary_max_token_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        summary_ids = summary_encoding[\"input_ids\"]\n",
        "        summary_ids[\n",
        "            summary_ids == 0\n",
        "        ] = (\n",
        "            -100\n",
        "        )  # Note: the input_ids includes padding too, so replace pad tokens(zero value) with value of -100\n",
        "\n",
        "        return dict(\n",
        "            text=text,\n",
        "            text_ids=text_encoding[\"input_ids\"].flatten(),\n",
        "            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n",
        "            summary=summary,\n",
        "            summary_ids=summary_ids.flatten(),\n",
        "            summary_attention_mask=summary_encoding[\"attention_mask\"].flatten(),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdlQbeyrV068"
      },
      "source": [
        "### DataModule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-Xr_9wvV069"
      },
      "outputs": [],
      "source": [
        "class DataModuleGenerator(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df,\n",
        "        valid_df,\n",
        "        test_df,\n",
        "        tokenizer,\n",
        "        batch_size,\n",
        "        text_max_token_len,\n",
        "        summary_max_token_len,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.test_df = test_df\n",
        "        self.batch_size = batch_size\n",
        "        self.text_max_token_len = text_max_token_len\n",
        "        self.summary_max_token_len = summary_max_token_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = CustomDataset(\n",
        "            self.train_df,\n",
        "            self.tokenizer,\n",
        "            self.text_max_token_len,\n",
        "            self.summary_max_token_len,\n",
        "        )\n",
        "\n",
        "        self.valid_dataset = CustomDataset(\n",
        "            self.valid_df,\n",
        "            self.tokenizer,\n",
        "            self.text_max_token_len,\n",
        "            self.summary_max_token_len,\n",
        "        )\n",
        "\n",
        "        self.test_dataset = CustomDataset(\n",
        "            self.test_df,\n",
        "            self.tokenizer,\n",
        "            self.text_max_token_len,\n",
        "            self.summary_max_token_len,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=1,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.valid_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=1,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=1,\n",
        "            pin_memory=True,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSdAt1hpV069"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpjiIKdkV069"
      },
      "outputs": [],
      "source": [
        "class MyModel(pl.LightningModule):\n",
        "    def __init__(self, tokenizer, config):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
        "            config.pretrained_model_name,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        # metrics\n",
        "        self.rouge_ja = RougeCalculator(stopwords=True, lang=\"ja\")\n",
        "        # self.bleu_ja = BLEUCalculator(lang=\"ja\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        text_ids,\n",
        "        text_attention_mask,\n",
        "        summary_ids=None,\n",
        "        summary_attention_mask=None,\n",
        "    ):\n",
        "        output = self.model(\n",
        "            text_ids,\n",
        "            attention_mask=text_attention_mask,\n",
        "            labels=summary_ids,\n",
        "            decoder_attention_mask=summary_attention_mask,\n",
        "        )  # loss func is cross entropy\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def predict(self, text_ids, text_attention_mask):\n",
        "        output = self.model.generate(\n",
        "            text_ids,\n",
        "            attention_mask=text_attention_mask,\n",
        "            max_length=self.config.data_module.summary_max_length,\n",
        "            num_beams=1,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "        return [\n",
        "            self.tokenizer.decode(\n",
        "                ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            for ids in output\n",
        "        ]\n",
        "\n",
        "    def _step(self, batch, return_text=False):\n",
        "        loss, logits = self(\n",
        "            text_ids=batch[\"text_ids\"],\n",
        "            text_attention_mask=batch[\"text_attention_mask\"],\n",
        "            summary_ids=batch[\"summary_ids\"],\n",
        "            summary_attention_mask=batch[\"summary_attention_mask\"],\n",
        "        )\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits,\n",
        "            \"text_ids\": batch[\"text_ids\"],\n",
        "            \"summary_ids\": batch[\"summary_ids\"],\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch, batch_size):\n",
        "        results = self._step(batch)\n",
        "        self.log(\"train/loss\", results[\"loss\"], prog_bar=True)\n",
        "        return results\n",
        "\n",
        "    def _val_test_step(self, batch, batch_size, mode=\"val\"):\n",
        "        results = self._step(batch)\n",
        "\n",
        "        predicted_texts = self.predict(batch[\"text_ids\"], batch[\"text_attention_mask\"])\n",
        "\n",
        "        metrics = {\n",
        "            \"rouge_1\": [],\n",
        "            \"rouge_2\": [],\n",
        "            \"rouge_l\": [],\n",
        "            # \"bleu\": [],\n",
        "        }\n",
        "        for text, summary, predicted_text in zip(\n",
        "            batch[\"text\"], batch[\"summary\"], predicted_texts\n",
        "        ):\n",
        "            metrics[\"rouge_1\"].append(\n",
        "                self.rouge_ja.rouge_n(summary, predicted_text, n=1)\n",
        "            )\n",
        "            metrics[\"rouge_2\"].append(\n",
        "                self.rouge_ja.rouge_n(summary, predicted_text, n=2)\n",
        "            )\n",
        "            metrics[\"rouge_l\"].append(self.rouge_ja.rouge_l(summary, predicted_text))\n",
        "            # metrics[\"bleu\"].append(self.bleu_ja.bleu(summary, predicted_text))\n",
        "\n",
        "        return {\n",
        "            \"loss\": results[\"loss\"],\n",
        "            \"rouge_1\": np.mean(metrics[\"rouge_1\"]),\n",
        "            \"rouge_2\": np.mean(metrics[\"rouge_2\"]),\n",
        "            \"rouge_l\": np.mean(metrics[\"rouge_l\"]),\n",
        "            # \"bleu\": np.mean(metrics[\"bleu\"]),\n",
        "            \"text\": batch[\"text\"],\n",
        "            \"summary\": batch[\"summary\"],\n",
        "            \"predicted_text\": predicted_texts,\n",
        "        }\n",
        "\n",
        "    def validation_step(self, batch, batch_size):\n",
        "        return self._val_test_step(batch, batch_size, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_size):\n",
        "        return self._val_test_step(batch, batch_size, mode=\"test\")\n",
        "\n",
        "    def _epoch_end(self, outputs, mode):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        self.log(f\"{mode}/loss\", avg_loss)\n",
        "\n",
        "        avg_rouge_1 = np.mean([x[\"rouge_1\"] for x in outputs])\n",
        "        avg_rouge_2 = np.mean([x[\"rouge_2\"] for x in outputs])\n",
        "        avg_rouge_l = np.mean([x[\"rouge_l\"] for x in outputs])\n",
        "        # avg_bleu = np.mean([x[\"bleu\"] for x in outputs])\n",
        "        self.log(f\"{mode}/rouge_1\", avg_rouge_1)\n",
        "        self.log(f\"{mode}/rouge_2\", avg_rouge_2)\n",
        "        self.log(f\"{mode}/rouge_l\", avg_rouge_l)\n",
        "        # self.log(f\"{mode}/bleu\", avg_bleu)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self._epoch_end(outputs, mode=\"val\")\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        self._epoch_end(outputs, mode=\"test\")\n",
        "        results = []\n",
        "        for step_output in outputs:\n",
        "            for text, summary, predicted_text in zip(\n",
        "                step_output[\"text\"],\n",
        "                step_output[\"summary\"],\n",
        "                step_output[\"predicted_text\"],\n",
        "            ):\n",
        "                results.append(\n",
        "                    [\n",
        "                        text,\n",
        "                        summary,\n",
        "                        predicted_text,\n",
        "                    ]\n",
        "                )\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"test/results\": wandb.Table(\n",
        "                    data=results, columns=[\"text\", \"summary\", \"predicted_text\"]\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        assert self.config.optimizer.name in [\"AdamW\", \"RAdam\"]\n",
        "        if self.config.optimizer.name == \"AdamW\":\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                self.parameters(),\n",
        "                lr=self.config.optimizer.lr,\n",
        "            )\n",
        "        elif self.config.optimizer.name == \"RAdam\":\n",
        "            optimizer = torch.optim.RAdam(\n",
        "                self.parameters(),\n",
        "                lr=self.config.optimizer.lr,\n",
        "            )\n",
        "        return [optimizer]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz5P_RFNV06-"
      },
      "source": [
        "## Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHaqnrVOV06-"
      },
      "outputs": [],
      "source": [
        "class MyTrainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def execute(self):\n",
        "        current = (datetime.datetime.now() + datetime.timedelta(hours=9)).strftime(\n",
        "            \"%Y%m%d_%H%M%S\"\n",
        "        )\n",
        "        MODEL_OUTPUT_DIR = \"/content/drive/MyDrive/MurataLab/summary/models/\" + current\n",
        "        os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        wandb.init(\n",
        "            project=self.config.wandb_project_name,\n",
        "            name=current,\n",
        "            config=self.config,\n",
        "            id=current,\n",
        "            save_code=True,\n",
        "        )\n",
        "        config = Box(dict(wandb.config))\n",
        "\n",
        "        tokenizer = T5Tokenizer.from_pretrained(config.pretrained_model_name)\n",
        "\n",
        "        train_df, val_df = train_test_split(\n",
        "            pd.read_csv(TRAIN_DATASET_PATH),\n",
        "            train_size=config.data.train_rate,\n",
        "            random_state=config.seed,\n",
        "        )\n",
        "        test_df = pd.read_csv(TEST_DATASET_PATH)\n",
        "\n",
        "        data_module = DataModuleGenerator(\n",
        "            train_df=train_df,\n",
        "            valid_df=val_df,\n",
        "            test_df=test_df,\n",
        "            tokenizer=tokenizer,\n",
        "            batch_size=config.data_module.batch_size,\n",
        "            text_max_token_len=config.data_module.text_max_length,\n",
        "            summary_max_token_len=config.data_module.summary_max_length,\n",
        "        )\n",
        "        data_module.setup()\n",
        "\n",
        "        model = MyModel(\n",
        "            tokenizer,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            **config.early_stopping,\n",
        "        )\n",
        "\n",
        "        wandb_logger = WandbLogger(\n",
        "            log_model=False,\n",
        "        )\n",
        "        wandb_logger.watch(model, log=\"all\")\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath=MODEL_OUTPUT_DIR,\n",
        "            **config.checkpoint,\n",
        "        )\n",
        "\n",
        "        progress_bar = RichProgressBar()\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config.epoch,\n",
        "            accelerator=\"auto\",\n",
        "            devices=\"auto\",\n",
        "            callbacks=[checkpoint_callback, early_stop_callback, progress_bar],\n",
        "            logger=wandb_logger,\n",
        "            deterministic=True,\n",
        "            # precision=16,\n",
        "            # accumulate_grad_batches=config.accumulate_grad_batches,\n",
        "        )\n",
        "\n",
        "        trainer.fit(model, data_module)\n",
        "\n",
        "        trainer.test(model, data_module)\n",
        "\n",
        "        wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuPZt1Cfs8Mp"
      },
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMYhSoCs8Mp"
      },
      "outputs": [],
      "source": [
        "DO_SWEEP = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeW9kXlxs8Mp"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    wandb_project_name=\"summary\",\n",
        "    pretrained_model_name=\"sonoisa/t5-base-japanese\",\n",
        "    epoch=10,\n",
        "    seed=40,\n",
        "    data_module=dict(\n",
        "        batch_size=2,\n",
        "        text_max_length=30,  # データセットの入力テキストは21~25字\n",
        "        summary_max_length=17,  # 20字を超えないようにxトークンとする\n",
        "    ),\n",
        "    optimizer=dict(\n",
        "        name=\"RAdam\",\n",
        "        lr=1e-5,\n",
        "    ),\n",
        "    early_stopping=dict(\n",
        "        monitor=\"val/loss\",\n",
        "        patience=3,\n",
        "        mode=\"min\",\n",
        "        min_delta=0.02,\n",
        "    ),\n",
        "    checkpoint=dict(\n",
        "        monitor=\"val/loss\",\n",
        "        mode=\"min\",\n",
        "        filename=\"{epoch}\",\n",
        "        verbose=True,\n",
        "    ),\n",
        "    data=dict(\n",
        "        train_rate=0.9,\n",
        "    ),\n",
        "    accumulate_grad_batches=4,\n",
        ")\n",
        "\n",
        "config = Box(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKspiCGbs8Mq"
      },
      "outputs": [],
      "source": [
        "sweep_config = dict(\n",
        "    method=\"random\",\n",
        "    metric=dict(\n",
        "        goal=\"minimize\",\n",
        "        name=\"val/loss\",\n",
        "    ),\n",
        "    parameters=dict(\n",
        "        data_module=dict(\n",
        "            parameters=dict(\n",
        "                batch_size=dict(\n",
        "                    values=[1, 2, 3, 4],\n",
        "                ),\n",
        "                text_max_length=25,  # データセットの入力テキストは21~25字\n",
        "                summary_max_length=17,\n",
        "            )\n",
        "        ),\n",
        "        optimizer=dict(\n",
        "            parameters=dict(\n",
        "                name=dict(\n",
        "                    values=[\"AdamW\", \"RAdam\"],\n",
        "                ),\n",
        "                lr=dict(\n",
        "                    values=[1e-5, 5e-5, 9e-5, 1e-6],\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGT-h9Hqs8Mq"
      },
      "source": [
        "## Execute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ErOwgAhbs8Mq",
        "outputId": "7defae9f-4a13-4fac-a73b-ba35ca21761a"
      },
      "outputs": [],
      "source": [
        "if DO_SWEEP:\n",
        "    sweep_id = wandb.sweep(sweep_config, project=config.wandb_project_name)\n",
        "    trainer = MyTrainer(config)\n",
        "    wandb.agent(sweep_id, trainer.execute, count=10)\n",
        "else:\n",
        "    trainer = MyTrainer(config)\n",
        "    trainer.execute()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Sp-2a-s8Mr"
      },
      "source": [
        "## Predict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo7KmAk2cHp4",
        "outputId": "1101be4b-1ab7-4888-ee29-70a9a61faaad"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = \"/content/drive/MyDrive/MurataLab/summary/models\"\n",
        "id = input(\"id (2023XXXX_XXXXXX) : \")\n",
        "epoch = input(\"epoch: \")\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(config.pretrained_model_name)\n",
        "\n",
        "trained_model = MyModel(\n",
        "    tokenizer,\n",
        "    config=config,\n",
        ")\n",
        "trained_model.load_state_dict(\n",
        "    torch.load(\n",
        "        os.path.join(MODEL_DIR, id, f\"epoch={epoch}.ckpt\"),\n",
        "        map_location=torch.device(\"cpu\"),\n",
        "    )[\"state_dict\"]\n",
        ")\n",
        "trained_model.eval()\n",
        "trained_model.freeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GlczhROccj8",
        "outputId": "5dfac8d6-0572-4f17-c42d-9145e1c6cd86"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    text = input(\"Text (exit): \")\n",
        "    if text == \"exit\":\n",
        "        break\n",
        "\n",
        "    t0 = time.time()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=config.data_module.text_max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    generated_ids = trained_model.model.generate(\n",
        "        input_ids=encoding[\"input_ids\"],\n",
        "        attention_mask=encoding[\"attention_mask\"],\n",
        "        max_length=config.data_module.summary_max_length,\n",
        "        num_beams=4,\n",
        "        repetition_penalty=2.5,\n",
        "        # length_penalty=1.0,\n",
        "        # early_stopping=True,\n",
        "    )\n",
        "    print(\"    Time: \", time.time() - t0)\n",
        "    print(f\"    {tokenizer.batch_decode(generated_ids, skip_special_tokens=True)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5 (main, Jul  7 2022, 20:58:07) [Clang 12.0.5 (clang-1205.0.22.11)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4476acf688e817183a1bbd96ced0d2d90c6104b36efc5ea62e2990d5ca0247a3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
