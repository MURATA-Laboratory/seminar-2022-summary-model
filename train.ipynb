{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 短縮モデル 学習ノートブック\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!pip install pytorch-lightning\n",
    "!pip install rich\n",
    "!pip install python-box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    wandb_project_name=\"summary\",\n",
    "    pretrained_model_name=\"sonoisa/t5-base-japanese\",\n",
    "    epoch=4,\n",
    "    seed=40,\n",
    "    data_module=dict(\n",
    "        batch_size=4,\n",
    "        text_max_length=30,\n",
    "        summary_max_length=17,  # not to exceed 20words\n",
    "    ),\n",
    "    optimizer=dict(\n",
    "        name=\"AdamW\",\n",
    "        lr=2e-5,\n",
    "        eps=1e-8,\n",
    "    ),\n",
    "    early_stopping=dict(\n",
    "        monitor=\"val/loss\",\n",
    "        patience=3,\n",
    "        mode=\"min\",\n",
    "        min_delta=0.02,\n",
    "    ),\n",
    "    checkpoint=dict(\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        filename=\"{epoch}\",\n",
    "        verbose=True,\n",
    "    ),\n",
    "    data=dict(\n",
    "        train_rate=0.8,\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = Box(config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(config.pretrained_model_name, is_fast=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"/content/drive/MyDrive/MurataLab/summary/train_dataset.csv\"\n",
    "TEST_DATASET_PATH = \"/content/drive/MyDrive/MurataLab/summary/test_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    pd.read_csv(TRAIN_DATASET_PATH),\n",
    "    train_size=config.data.train_rate,\n",
    "    random_state=config.seed,\n",
    ")\n",
    "test_df = pd.read_csv(TEST_DATASET_PATH)\n",
    "print(len(train_df), len(val_df), test_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    TEXT_COLUMN = \"text\"\n",
    "    SUMMARY_COLUMN = \"summary\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, text_max_token_len, summary_max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text, summary = data_row[self.TEXT_COLUMN], data_row[self.SUMMARY_COLUMN]\n",
    "\n",
    "        text_encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        summary_encoding = self.tokenizer.encode_plus(\n",
    "            summary,\n",
    "            max_length=self.summary_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        labels = summary_encoding[\"input_ids\"]\n",
    "        labels[labels==0] = -100 # Note: the input_ids includes padding too, so replace pad tokens(zero value) with value of -100\n",
    "\n",
    "        return dict(\n",
    "            text=text,\n",
    "            text_input_ids=text_encoding[\"input_ids\"].flatten(),\n",
    "            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n",
    "            summary=summary,\n",
    "            labels=labels.flatten(),\n",
    "            lebels_attention_mask=summary_encoding[\"attention_mask\"].flatten(),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleGenerator(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df,\n",
    "        valid_df,\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        batch_size,\n",
    "        text_max_token_len,\n",
    "        summary_max_token_len,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CustomDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len,\n",
    "        )\n",
    "\n",
    "        self.valid_dataset = CustomDataset(\n",
    "            self.valid_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len,\n",
    "        )\n",
    "\n",
    "        self.test_dataset = CustomDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=os.cpu_count() or 1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count() or 1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=os.cpu_count() or 1,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "data_module = DataModuleGenerator(\n",
    "    train_df=train_df,\n",
    "    valid_df=val_df,\n",
    "    test_df=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config.data_module.batch_size,\n",
    "    text_max_token_len=config.data_module.text_max_length,\n",
    "    summary_max_token_len=config.data_module.summary_max_length,\n",
    ")\n",
    "data_module.setup()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            config.pretrained_model_name,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_size):\n",
    "        loss, logits = self(\n",
    "            input_ids=batch[\"text_input_ids\"],\n",
    "            attention_mask=batch[\"text_attention_mask\"],\n",
    "            decoder_attention_mask=batch[\"labels_attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_size):\n",
    "        loss, logits = self(\n",
    "            input_ids=batch[\"text_input_ids\"],\n",
    "            attention_mask=batch[\"text_attention_mask\"],\n",
    "            decoder_attention_mask=batch[\"labels_attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_size):\n",
    "        loss, logits = self(\n",
    "            input_ids=batch[\"text_input_ids\"],\n",
    "            attention_mask=batch[\"text_attention_mask\"],\n",
    "            decoder_attention_mask=batch[\"labels_attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        assert self.config.optimizer.name in [\"AdamW\"]\n",
    "        if self.config.optimizer.name == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=self.config.optimizer.lr,\n",
    "                eps=self.config.optimizer.eps,\n",
    "            )\n",
    "        return [optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    tokenizer,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    **config.early_stopping,\n",
    ")\n",
    "\n",
    "current = (datetime.datetime.now() + datetime.timedelta(hours=9)).strftime(\n",
    "    \"%Y%m%d_%H%M%S\"\n",
    ")\n",
    "MODEL_OUTPUT_DIR = \"/content/drive/MyDrive/MurataLab/summary/models/\" + current\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=MODEL_OUTPUT_DIR,\n",
    "    **config.checkpoint,\n",
    ")\n",
    "\n",
    "\n",
    "progress_bar = RichProgressBar()\n",
    "\n",
    "wandb.init(\n",
    "    project=config.wandb_project_name,\n",
    "    name=current,\n",
    "    config=config,\n",
    "    id=current,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.epoch,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, progress_bar],\n",
    "    logger=WandbLogger(\n",
    "        log_model=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(datamodule=data_module, ckpt_path=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5 (main, Jul  7 2022, 20:58:07) [Clang 12.0.5 (clang-1205.0.22.11)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4476acf688e817183a1bbd96ced0d2d90c6104b36efc5ea62e2990d5ca0247a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
